{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from __future__ import division\n",
    "from numpy import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Reading the training data set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id22965</td>\n",
       "      <td>A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id09674</td>\n",
       "      <td>The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id13515</td>\n",
       "      <td>The surcingle hung in ribands from my body.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id19322</td>\n",
       "      <td>I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id00912</td>\n",
       "      <td>I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id26305   \n",
       "1  id17569   \n",
       "2  id11008   \n",
       "3  id27763   \n",
       "4  id12958   \n",
       "5  id22965   \n",
       "6  id09674   \n",
       "7  id13515   \n",
       "8  id19322   \n",
       "9  id00912   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \\\n",
       "0                                                                                                                                                                                                                                                                   This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                   It never once occurred to me that the fumbling might be a mere mistake.   \n",
       "2                                                                                                                                                                                                                                                                                                  In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.   \n",
       "3                                                                                                                                                                                                                                                                                            How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.   \n",
       "4                                                                                                                                                                                                                                                                                                                            Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.   \n",
       "5                      A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                          The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                               The surcingle hung in ribands from my body.   \n",
       "8  I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                          I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.   \n",
       "\n",
       "  author  \n",
       "0    EAP  \n",
       "1    HPL  \n",
       "2    EAP  \n",
       "3    MWS  \n",
       "4    HPL  \n",
       "5    MWS  \n",
       "6    EAP  \n",
       "7    EAP  \n",
       "8    EAP  \n",
       "9    MWS  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19579"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub=pd.read_csv(\"test.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total=pd.concat([df,df_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HPL</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EAP</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWS</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HPL</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MWS</td>\n",
       "      <td>id22965</td>\n",
       "      <td>A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EAP</td>\n",
       "      <td>id09674</td>\n",
       "      <td>The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EAP</td>\n",
       "      <td>id13515</td>\n",
       "      <td>The surcingle hung in ribands from my body.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EAP</td>\n",
       "      <td>id19322</td>\n",
       "      <td>I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MWS</td>\n",
       "      <td>id00912</td>\n",
       "      <td>I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author       id  \\\n",
       "0    EAP  id26305   \n",
       "1    HPL  id17569   \n",
       "2    EAP  id11008   \n",
       "3    MWS  id27763   \n",
       "4    HPL  id12958   \n",
       "5    MWS  id22965   \n",
       "6    EAP  id09674   \n",
       "7    EAP  id13515   \n",
       "8    EAP  id19322   \n",
       "9    MWS  id00912   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \n",
       "0                                                                                                                                                                                                                                                                   This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                   It never once occurred to me that the fumbling might be a mere mistake.  \n",
       "2                                                                                                                                                                                                                                                                                                  In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.  \n",
       "3                                                                                                                                                                                                                                                                                            How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.  \n",
       "4                                                                                                                                                                                                                                                                                                                            Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.  \n",
       "5                      A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                          The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                               The surcingle hung in ribands from my body.  \n",
       "8  I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                          I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling Approach:**\n",
    "    \n",
    "  1) Tfidf vectorizer and Count verctorizer will extract various word based features within the text, we can predict the author  based  on these word based features.\n",
    "  \n",
    "   2)Meta features such as sentence length , punctutaion frequency , max length of words in sentence will be used to build a predictive model.\n",
    "   \n",
    "   3) Word pattern based on Markov chains will be used to develop a predictve model.\n",
    "   \n",
    "   4) Finally a stacked model from Tfidf Vectors , Countvectorizer,Markov Chain ,Meta features and LSI features will be trained on XG boost to  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TfIdf Vectorrizer **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfIdf vectorizer converts the text into word based features. It will develop a matrix of shape(n_docs,vocab size). The matrix will be filled with word frequency * log(Total Number documents/Number of documents in which this word is present). By normalizing with the invrese log of the document frequecy, the word frequency will be normalised take care of features that are present in all documents and are less predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer=TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total=Vectorizer.fit_transform(df_total['text'].get_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Vectorizer.transform(df['text'].get_values())\n",
    "X_sub=Vectorizer.transform(df_sub['text'].get_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27971x27996 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 313932 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and train Data seperation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target']=df['author'].replace(['EAP','HPL','MWS'],[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df['Target'].get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,stratify=Y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 0.75, 1, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='log_loss', verbose=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb=MultinomialNB()\n",
    "parameters={'alpha':[0.0001,0.001,0.01,0.1,0.5,0.75,1,5,10]}\n",
    "clf=GridSearchCV(mnb,cv=5,param_grid=parameters,scoring='log_loss')\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.01}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8126659856996936"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4604502568209501"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test,clf.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calibration of TfIdf Vectorizer model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities given by Naive bayes algorithm are not accurate.So, calibrating the probabilities imporve the confidence of probabilities thrown by out by the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ca,X_test_va,Y_test_ca,Y_test_va=train_test_split(X_test,Y_test,test_size=0.5,stratify=Y_test,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_clf=CalibratedClassifierCV(clf,method='sigmoid',cv='prefit')\n",
    "calib_clf.fit(X_test_ca,Y_test_ca)\n",
    "calibrated_proba=calib_clf.predict_proba(X_test_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49930310623484386"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test_va,calibrated_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222313036371066"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test_ca,calib_clf.predict(X_test_ca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of both the calibrated and base model is almost the same.However, the logloss score has dropped for the calibrated \n",
    "classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Count Vectorizer Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='log_loss', verbose=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt=CountVectorizer()\n",
    "cvt.fit(df_total['text'].get_values())\n",
    "Xcvt=cvt.transform(df['text'].get_values())\n",
    "Ycvt=df['Target'].get_values()\n",
    "Xcvt_train,Xcvt_test,Ycvt_train,Ycvt_test=train_test_split(Xcvt,Ycvt,test_size=0.25,stratify=Y,random_state=42)\n",
    "parameters_cvt={'alpha':[0.001,0.01,0.1,1,10,100]}\n",
    "Mnb2=MultinomialNB()\n",
    "clf_cvt=GridSearchCV(Mnb2,cv=5,param_grid=parameters_cvt,scoring='log_loss')\n",
    "clf_cvt.fit(Xcvt_train,Ycvt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4810625307001191"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Ycvt_test,clf_cvt.predict_proba(Xcvt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.839223697650664"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Ycvt_test,clf_cvt.predict(Xcvt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cvt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calibrating probabilities for Count vectorizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcvt_train_ca,Xcvt_test_ca,Ycvt_train_ca,Ycvt_test_ca=train_test_split(Xcvt,Ycvt,test_size=0.25,stratify=Ycvt,random_state=42)\n",
    "clf_cvt_calib=CalibratedClassifierCV(clf_cvt,method='sigmoid',cv='prefit')\n",
    "clf_cvt_calib.fit(Xcvt_train_ca,Ycvt_train_ca)\n",
    "cvt_ca_proba=clf_cvt_calib.predict_proba(Xcvt_test_ca)\n",
    "cvt_ca_predict=clf_cvt_calib.predict(Xcvt_test_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4833647175025266"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Ycvt_test_ca,cvt_ca_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8406537282941777"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Ycvt_test_ca,cvt_ca_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over allthe Calibrated Count vectorizer model has higher accuracy of 84 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Markov Chain Models **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Naive bayes algorithm, we assume that the features are independent. This assumption might not be correct for all data sets. On the other hand,in Markov models of order n, we assume that each feature is dependent on n preceeding features. In the current context, each author might have specific usage of words and punctutaions. This will be visible in the conditional frequency of a character based on the previous n characters. How ever, we are not sure which n will work for this specific problem. So, the best option is to try differnt (n) and shall select the one that gives best prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id22965</td>\n",
       "      <td>A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id09674</td>\n",
       "      <td>The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id13515</td>\n",
       "      <td>The surcingle hung in ribands from my body.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id19322</td>\n",
       "      <td>I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id00912</td>\n",
       "      <td>I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id26305   \n",
       "1  id17569   \n",
       "2  id11008   \n",
       "3  id27763   \n",
       "4  id12958   \n",
       "5  id22965   \n",
       "6  id09674   \n",
       "7  id13515   \n",
       "8  id19322   \n",
       "9  id00912   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \\\n",
       "0                                                                                                                                                                                                                                                                   This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                   It never once occurred to me that the fumbling might be a mere mistake.   \n",
       "2                                                                                                                                                                                                                                                                                                  In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.   \n",
       "3                                                                                                                                                                                                                                                                                            How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.   \n",
       "4                                                                                                                                                                                                                                                                                                                            Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.   \n",
       "5                      A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                          The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                               The surcingle hung in ribands from my body.   \n",
       "8  I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                          I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.   \n",
       "\n",
       "  author  Target  \n",
       "0    EAP       0  \n",
       "1    HPL       1  \n",
       "2    EAP       0  \n",
       "3    MWS       2  \n",
       "4    HPL       1  \n",
       "5    MWS       2  \n",
       "6    EAP       0  \n",
       "7    EAP       0  \n",
       "8    EAP       0  \n",
       "9    MWS       2  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['id','text','author','Target']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train and Test split for Markov models **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmk_train,Xmk_test,Ymk_train,Ymk_test=train_test_split(df['text'].get_values(),df['author'].get_values(),test_size=0.25,stratify=Ycvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Function to create character n grams **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conditional_n_grams(text_columns_list,class_column_list,ngram):\n",
    "    ###################################################################################\n",
    "    def Create_Char_n_Grams(string,n):\n",
    "        text=(string)\n",
    "        gram_len=n\n",
    "        list_ngrams=[]\n",
    "        for p in np.arange(len(text)):\n",
    "            if p+gram_len>len(text):\n",
    "                break\n",
    "            else:    \n",
    "                list_ngrams.append(text[p:p+gram_len])\n",
    "        return(list_ngrams)\n",
    "    Vector_Char_n_Grams=np.vectorize(Create_Char_n_Grams)\n",
    "    ##################################################################################################\n",
    "    df=pd.DataFrame({'text':text_columns_list,'class':class_column_list})\n",
    "    df['Char_n_Grams']=df['text'].apply(lambda x : Vector_Char_n_Grams(x,ngram))\n",
    "    list_ngram=[]\n",
    "    list_author=[]\n",
    "    for k in df['class'].unique():\n",
    "        list_Char_n_Grams=list(itertools.chain.from_iterable(list(df['Char_n_Grams'][df['class']==k])))\n",
    "        list_Author=[k]*len(list_Char_n_Grams)\n",
    "        list_ngram.append(list_Char_n_Grams)\n",
    "        list_author.append(list_Author)\n",
    "    list_ngram=list(itertools.chain.from_iterable(list_ngram))\n",
    "    list_author=list(itertools.chain.from_iterable(list_author))\n",
    "    df_n_grams=pd.DataFrame({'Ngram':list_ngram,'Author':list_author})\n",
    "    df_n_grams['Precedent']=df_n_grams['Ngram'].apply(lambda x : x[:-1])\n",
    "    df_n_grams['Antecedent']=df_n_grams['Ngram'].apply(lambda x : x[-1:])\n",
    "    df_n_grams['Ngram_Length']=df_n_grams['Ngram'].apply(lambda x : len(x))\n",
    "    df_n_grams['Precedent_Count']=df_n_grams.groupby(['Author','Precedent'])['Ngram'].transform(lambda x : len(x))\n",
    "    df_n_grams['Cond_Antecedent_Count']=df_n_grams.groupby(['Author','Precedent','Antecedent'])['Ngram'].transform(lambda x : len(x))\n",
    "    df_n_grams['Conditional_Probability']=df_n_grams['Cond_Antecedent_Count']/df_n_grams['Precedent_Count']\n",
    "    df_n_grams.drop_duplicates(keep='first',inplace=True)\n",
    "    return(df_n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=Conditional_n_grams(list(Xmk_train),list(Ymk_train),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Ngram</th>\n",
       "      <th>Precedent</th>\n",
       "      <th>Antecedent</th>\n",
       "      <th>Ngram_Length</th>\n",
       "      <th>Precedent_Count</th>\n",
       "      <th>Cond_Antecedent_Count</th>\n",
       "      <th>Conditional_Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HPL</td>\n",
       "      <td>Upon</td>\n",
       "      <td>Upo</td>\n",
       "      <td>n</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HPL</td>\n",
       "      <td>pon</td>\n",
       "      <td>pon</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>186</td>\n",
       "      <td>138</td>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HPL</td>\n",
       "      <td>on t</td>\n",
       "      <td>on</td>\n",
       "      <td>t</td>\n",
       "      <td>4</td>\n",
       "      <td>1839</td>\n",
       "      <td>511</td>\n",
       "      <td>0.277868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HPL</td>\n",
       "      <td>n th</td>\n",
       "      <td>n t</td>\n",
       "      <td>h</td>\n",
       "      <td>4</td>\n",
       "      <td>2066</td>\n",
       "      <td>1720</td>\n",
       "      <td>0.832527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HPL</td>\n",
       "      <td>the</td>\n",
       "      <td>th</td>\n",
       "      <td>e</td>\n",
       "      <td>4</td>\n",
       "      <td>12271</td>\n",
       "      <td>9060</td>\n",
       "      <td>0.738326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HPL</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>9772</td>\n",
       "      <td>7719</td>\n",
       "      <td>0.789910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HPL</td>\n",
       "      <td>he d</td>\n",
       "      <td>he</td>\n",
       "      <td>d</td>\n",
       "      <td>4</td>\n",
       "      <td>9210</td>\n",
       "      <td>496</td>\n",
       "      <td>0.053855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HPL</td>\n",
       "      <td>e de</td>\n",
       "      <td>e d</td>\n",
       "      <td>e</td>\n",
       "      <td>4</td>\n",
       "      <td>833</td>\n",
       "      <td>231</td>\n",
       "      <td>0.277311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HPL</td>\n",
       "      <td>dea</td>\n",
       "      <td>de</td>\n",
       "      <td>a</td>\n",
       "      <td>4</td>\n",
       "      <td>999</td>\n",
       "      <td>154</td>\n",
       "      <td>0.154154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HPL</td>\n",
       "      <td>deat</td>\n",
       "      <td>dea</td>\n",
       "      <td>t</td>\n",
       "      <td>4</td>\n",
       "      <td>184</td>\n",
       "      <td>75</td>\n",
       "      <td>0.407609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Author Ngram Precedent Antecedent  Ngram_Length  Precedent_Count  \\\n",
       "0    HPL  Upon       Upo          n             4               15   \n",
       "1    HPL  pon        pon                        4              186   \n",
       "2    HPL  on t       on           t             4             1839   \n",
       "3    HPL  n th       n t          h             4             2066   \n",
       "4    HPL   the        th          e             4            12271   \n",
       "5    HPL  the        the                        4             9772   \n",
       "6    HPL  he d       he           d             4             9210   \n",
       "7    HPL  e de       e d          e             4              833   \n",
       "8    HPL   dea        de          a             4              999   \n",
       "9    HPL  deat       dea          t             4              184   \n",
       "\n",
       "   Cond_Antecedent_Count  Conditional_Probability  \n",
       "0                     15                 1.000000  \n",
       "1                    138                 0.741935  \n",
       "2                    511                 0.277868  \n",
       "3                   1720                 0.832527  \n",
       "4                   9060                 0.738326  \n",
       "5                   7719                 0.789910  \n",
       "6                    496                 0.053855  \n",
       "7                    231                 0.277311  \n",
       "8                    154                 0.154154  \n",
       "9                     75                 0.407609  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predicting for a sentence **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Target_Prediction(list_text,ngram,prior=dataframe,alpha=0.1):\n",
    "    \n",
    "    def Create_Char_n_Grams(string,n):\n",
    "        text=(string)\n",
    "        gram_len=n\n",
    "        list_ngrams=[]\n",
    "        for p in np.arange(len(text)):\n",
    "            if p+gram_len>len(text):\n",
    "                break\n",
    "            else:    \n",
    "                list_ngrams.append(text[p:p+gram_len])\n",
    "        return(list_ngrams)\n",
    "    Vector_Char_n_Grams=np.vectorize(Create_Char_n_Grams)\n",
    "    #########################################################################################################################\n",
    "    df_text=pd.DataFrame({'Text':list_text})\n",
    "    df_text['list_ngram']=df_text['Text'].apply(lambda x : Vector_Char_n_Grams(x,ngram))\n",
    "   ##########################################################################################################################\n",
    "    def Changing_to_Data_Frame(list_text1,prior1=prior,alpha=0.1):\n",
    "        df_EAP=prior1[prior1['Author']=='EAP']\n",
    "        df_HPL=prior1[prior1['Author']=='HPL']\n",
    "        df_MWS=prior1[prior1['Author']=='MWS']\n",
    "        df_test=pd.DataFrame({'Ngram':list_text1})\n",
    "        df_EAP_temp=pd.merge(df_test,df_EAP,left_on='Ngram',right_on='Ngram',how='left')\n",
    "        df_EAP_temp=df_EAP_temp[['Author','Ngram','Conditional_Probability']]\n",
    "        df_EAP_temp['Conditional_Probability'][df_EAP_temp['Conditional_Probability'].isnull()]=alpha\n",
    "        df_HPL_temp=pd.merge(df_test,df_HPL,left_on='Ngram',right_on='Ngram',how='left')\n",
    "        df_HPL_temp=df_HPL_temp[['Author','Ngram','Conditional_Probability']]\n",
    "        df_HPL_temp['Conditional_Probability'][df_HPL_temp['Conditional_Probability'].isnull()]=alpha\n",
    "        df_MWS_temp=pd.merge(df_test,df_MWS,left_on='Ngram',right_on='Ngram',how='left')\n",
    "        df_MWS_temp=df_MWS_temp[['Author','Ngram','Conditional_Probability']]\n",
    "        df_MWS_temp['Conditional_Probability'][df_MWS_temp['Conditional_Probability'].isnull()]=alpha\n",
    "        EAP=sum(np.log(df_EAP_temp['Conditional_Probability']))+np.log(1/3)\n",
    "        HPL=sum(np.log(df_HPL_temp['Conditional_Probability']))+np.log(1/3)\n",
    "        MWS=sum(np.log(df_MWS_temp['Conditional_Probability']))+np.log(1/3)\n",
    "        probability_EAP=1/(1+np.exp(HPL-EAP)+np.exp(MWS-EAP))\n",
    "        probability_HPL=1/(1+np.exp(EAP-HPL)+np.exp(MWS-HPL))\n",
    "        probability_MWS=1/(1+np.exp(EAP-MWS)+np.exp(HPL-MWS))\n",
    "        return(probability_EAP,probability_HPL,probability_MWS)\n",
    "    #Vectorize_Changing_to_Data_Frame=np.vectorize(Changing_to_Data_Frame)\n",
    "  ###########################################################################################################################\n",
    "    EAP_probability=[]\n",
    "    HPL_probability=[]\n",
    "    MWS_probability=[]\n",
    "    count=0\n",
    "    for k in df_text['list_ngram']:\n",
    "        A1,A2,A3=Changing_to_Data_Frame(k)\n",
    "        count+=1\n",
    "        if count%1000==0:\n",
    "            print(count)\n",
    "        EAP_probability.append(A1)\n",
    "        HPL_probability.append(A2)\n",
    "        MWS_probability.append(A3)\n",
    "    df_output=pd.DataFrame({'EAP':EAP_probability,'HPL':HPL_probability,'MWS':MWS_probability}) \n",
    "    #df_text['Probabilities']=df_text['list_ngram'].apply(lambda x : Vectorize_Changing_to_Data_Frame(x))\n",
    "   ##########################################################################################################################\n",
    "    #output_array=Vectorize_Changing_to_Data_Frame(df_text['list_ngram'].get_values())\n",
    "    \n",
    "    return(df_output.get_values())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "mk_prediction=Target_Prediction(Xmk_test,4,prior=dataframe,alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_predi_labels=np.argmax(mk_prediction.get_values(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ymk_test_labels=[]\n",
    "for p in Ymk_test:\n",
    "    if p=='EAP':\n",
    "        Ymk_test_labels.append(0) \n",
    "    elif p=='HPL':\n",
    "        Ymk_test_labels.append(1)\n",
    "    else:\n",
    "        Ymk_test_labels.append(2)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789785495403473"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Ymk_test_labels,mk_predi_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0075777542162356"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Ymk_test_labels,mk_prediction.get_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov models are giving an accuracy of 79% which is close to the prediction accuracy of TfIdf and Count vectorizer based models.\n",
    "But the log loss is very low compared to both of them. \n",
    "\n",
    "We can try different n gram size and alpha values and see if it imporves the prediction accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacked Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id26305   \n",
       "1  id17569   \n",
       "2  id11008   \n",
       "3  id27763   \n",
       "4  id12958   \n",
       "\n",
       "                                                                                                                                                                                                                                      text  \\\n",
       "0  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.   \n",
       "1                                                                                                                                                                  It never once occurred to me that the fumbling might be a mere mistake.   \n",
       "2                                 In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.   \n",
       "3                           How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.   \n",
       "4                                                           Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.   \n",
       "\n",
       "  author  Target  \n",
       "0    EAP       0  \n",
       "1    HPL       1  \n",
       "2    EAP       0  \n",
       "3    MWS       2  \n",
       "4    HPL       1  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack=df['text'].get_values()\n",
    "Y_stack=df['Target'].get_values()\n",
    "Y_author_stack=df['author'].get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "1000\n",
      "2000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "Skf=StratifiedKFold(n_splits=5,shuffle=False,random_state=42)\n",
    "X_tfidf_stack=np.zeros((Y_stack.shape[0],2))\n",
    "X_cvt_stack=np.zeros((Y_stack.shape[0],2))\n",
    "X_mk_stack=np.zeros((Y_stack.shape[0],2))\n",
    "for train_index,test_index in Skf.split(X_stack,Y_stack):\n",
    "    #Multinominal Naive Bayes Model based on TfIdf\n",
    "    ######################################################################################################################\n",
    "    Vectorizer=TfidfVectorizer(stop_words='english')\n",
    "    X_train_tfidf=Vectorizer.fit_transform(X_stack[train_index])\n",
    "    Y_train_tfidf=Y_stack[train_index]\n",
    "    X_test_tfidf=Vectorizer.transform(X_stack[test_index])\n",
    "    mnb_tfidf=MultinomialNB(alpha=0.01)\n",
    "    mnb_tfidf.fit(X_train_tfidf,Y_train_tfidf)\n",
    "    predict_probability=mnb_tfidf.predict_proba(X_test_tfidf)\n",
    "    X_tfidf_stack[test_index,0]=predict_probability[:,0]\n",
    "    X_tfidf_stack[test_index,1]=predict_probability[:,1]\n",
    "   #Multinominal Naive Bayes based on CountVectorizer\n",
    "    #####################################################################################################################\n",
    "    cvt_Vectorizer=CountVectorizer()\n",
    "    X_train_cvt=cvt_Vectorizer.fit_transform(X_stack[train_index])\n",
    "    Y_train_cvt=Y_stack[train_index]\n",
    "    X_test_cvt=cvt_Vectorizer.transform(X_stack[test_index])\n",
    "    mnb_cvt=MultinomialNB(alpha=1)\n",
    "    mnb_cvt.fit(X_train_cvt,Y_train_cvt)\n",
    "    predict_proba_cvt=mnb_cvt.predict_proba(X_test_cvt)\n",
    "    X_cvt_stack[test_index,0]=predict_proba_cvt[:,0]\n",
    "    X_cvt_stack[test_index,1]=predict_proba_cvt[:,1]\n",
    "   #####################################################################################################################\n",
    "    dataframe=Conditional_n_grams(list(X_stack[train_index]),list(Y_author_stack[train_index]),4)\n",
    "    predict_proba_mk=Target_Prediction(X_stack[test_index],4,prior=dataframe,alpha=0.1)\n",
    "    X_mk_stack[test_index,0]=predict_proba_mk[:,0]\n",
    "    X_mk_stack[test_index,1]=predict_proba_mk[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TfidfNb=pd.DataFrame(X_tfidf_stack,columns=['Tfidf_feat1','Tfidf_feat2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TfidfNb['Tfidf_feat1']=df_TfidfNb['Tfidf_feat1'].apply(lambda x : np.round(x,decimals=2))\n",
    "df_TfidfNb['Tfidf_feat2']=df_TfidfNb['Tfidf_feat2'].apply(lambda x : np.round(x,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tfidf_feat1</th>\n",
       "      <th>Tfidf_feat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tfidf_feat1  Tfidf_feat2\n",
       "0         0.99         0.00\n",
       "1         0.20         0.76\n",
       "2         0.96         0.04\n",
       "3         0.00         0.00\n",
       "4         0.61         0.36\n",
       "5         0.00         0.00\n",
       "6         0.79         0.14\n",
       "7         0.98         0.01\n",
       "8         0.98         0.02\n",
       "9         0.29         0.03"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TfidfNb.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvtNb=pd.DataFrame(X_cvt_stack,columns=['cvtNb_feat1','cvtNb_feat2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvtNb['cvtNb_feat1']=df_cvtNb['cvtNb_feat1'].apply(lambda x :np.round(x,2))\n",
    "df_cvtNb['cvtNb_feat2']=df_cvtNb['cvtNb_feat2'].apply(lambda x :np.round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvtNb_feat1</th>\n",
       "      <th>cvtNb_feat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cvtNb_feat1  cvtNb_feat2\n",
       "0         1.00         0.00\n",
       "1         0.88         0.09\n",
       "2         0.98         0.02\n",
       "3         0.00         0.00\n",
       "4         0.63         0.37\n",
       "5         0.00         0.00\n",
       "6         0.99         0.01\n",
       "7         0.93         0.04\n",
       "8         1.00         0.00\n",
       "9         0.16         0.02"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cvtNb.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk=pd.DataFrame(X_mk_stack,columns=['mk_feat1','mk_feat2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk['mk_feat1']=df_mk['mk_feat1'].apply(lambda x :np.round(x,2))\n",
    "df_mk['mk_feat2']=df_mk['mk_feat2'].apply(lambda x :np.round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mk_feat1</th>\n",
       "      <th>mk_feat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mk_feat1  mk_feat2\n",
       "0      1.00      0.00\n",
       "1      0.00      1.00\n",
       "2      1.00      0.00\n",
       "3      0.00      0.00\n",
       "4      0.53      0.46\n",
       "5      0.00      0.00\n",
       "6      1.00      0.00\n",
       "7      0.81      0.10\n",
       "8      1.00      0.00\n",
       "9      0.00      0.00"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mk.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked=pd.concat([df_TfidfNb,df_cvtNb,df_mk],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tfidf_feat1</th>\n",
       "      <th>Tfidf_feat2</th>\n",
       "      <th>cvtNb_feat1</th>\n",
       "      <th>cvtNb_feat2</th>\n",
       "      <th>mk_feat1</th>\n",
       "      <th>mk_feat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tfidf_feat1  Tfidf_feat2  cvtNb_feat1  cvtNb_feat2  mk_feat1  mk_feat2\n",
       "0         0.99         0.00         1.00         0.00      1.00      0.00\n",
       "1         0.20         0.76         0.88         0.09      0.00      1.00\n",
       "2         0.96         0.04         0.98         0.02      1.00      0.00\n",
       "3         0.00         0.00         0.00         0.00      0.00      0.00\n",
       "4         0.61         0.36         0.63         0.37      0.53      0.46\n",
       "5         0.00         0.00         0.00         0.00      0.00      0.00\n",
       "6         0.79         0.14         0.99         0.01      1.00      0.00\n",
       "7         0.98         0.01         0.93         0.04      0.81      0.10\n",
       "8         0.98         0.02         1.00         0.00      1.00      0.00\n",
       "9         0.29         0.03         0.16         0.02      0.00      0.00"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stacked.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features from Latent Semantec Indexing **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf vectorizers generates sparse matrices.By using svd, we compress the over all dimensionality to 100 and in those dimensions, each document will be represented as a data point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_stack=TfidfVectorizer(stop_words='english')\n",
    "X_stack_Tfidf=Vectorizer_stack.fit_transform(X_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd=TruncatedSVD(n_components=30,algorithm='arpack',random_state=42)\n",
    "X_svd=svd.fit_transform(X_stack_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_columns=[]\n",
    "for p in np.arange(30):\n",
    "    svd_columns.append('svd_columns_'+str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svd_features=pd.DataFrame(X_svd,columns=svd_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Meta features **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each sentence we will extract various meta features such as sentence length, max size of words ,number of punctuations,\n",
    "number of each punctuation,number of adjectives,number of words unique to each author. All these features will be added to other\n",
    "features to build the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_words']=df['text'].apply(lambda x : len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Period']=df['text'].str.count('\\.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Comma']=df['text'].str.count(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Question_Marks']=df['text'].str.count(r'\\?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Colon']=df['text'].str.count(r':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Semicolon']=df['text'].str.count(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quotation_Mark']=df['text'].str.count('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Aphostrophe']=df['text'].str.count(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_title_words=[]\n",
    "for sentence in df['text']:\n",
    "    doc=nlp(sentence)\n",
    "    p=0\n",
    "    for tokens in doc:\n",
    "        if tokens.text.istitle() :\n",
    "            p=p+1\n",
    "    number_of_title_words.append(p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_Case_Words']=number_of_title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta=df[['n_words','Period','Comma','Question_Marks','Colon','Semicolon','Quotation_Mark','Aphostrophe','Title_Case_Words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack=pd.concat([df_meta,df_svd_features,df_stacked],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 45)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579,)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_stack,X_test_stack,Y_training_stack,Y_test_stack=train_test_split(df_stack.get_values(),df['Target'].get_values(),test_size=0.25,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14684, 45)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4895, 45)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14684,)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4895,)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Features for submission data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with such inquietude and impatience, my father thought it best to yield.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be fanned with a newspaper, and as the government grew weaker, I have no doubt that leather and iron acquired durability in proportion, for, in a very short time, there was not a pair of bellows in all Rotterdam that ever stood in need of a stitch or required the assistance of a hammer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door they found only this: two cleanly picked human skeletons on the earthen floor, and a number of singular beetles crawling in the shadowy corners.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly manage without them, one actually tumbled out of my head, and, rolling down the steep side of the steeple, lodged in the rain gutter which ran along the eaves of the main building.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may extend.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id02310   \n",
       "1  id24541   \n",
       "2  id00134   \n",
       "3  id27757   \n",
       "4  id04081   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                         text  \n",
       "0                                                                                                                                                                                                                              Still, as I urged our leaving Ireland with such inquietude and impatience, my father thought it best to yield.  \n",
       "1  If a fire wanted fanning, it could readily be fanned with a newspaper, and as the government grew weaker, I have no doubt that leather and iron acquired durability in proportion, for, in a very short time, there was not a pair of bellows in all Rotterdam that ever stood in need of a stitch or required the assistance of a hammer.  \n",
       "2                                                                                                                                               And when they had broken down the frail door they found only this: two cleanly picked human skeletons on the earthen floor, and a number of singular beetles crawling in the shadowy corners.  \n",
       "3                                                                                                             While I was thinking how I should possibly manage without them, one actually tumbled out of my head, and, rolling down the steep side of the steeple, lodged in the rain gutter which ran along the eaves of the main building.  \n",
       "4                                                                                                                                                                                                                                                                                       I am not sure to what limit his knowledge may extend.  "
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TfIdf features for submission data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer=TfidfVectorizer(stop_words='english')\n",
    "X_train_stack=Vectorizer.fit_transform(df['text'].get_values())\n",
    "Y_train_stack=df['Target'].get_values()\n",
    "X_sub_stack=Vectorizer.transform(df_sub['text'].get_values())\n",
    "mnb_train=MultinomialNB(alpha=0.01)\n",
    "mnb_train.fit(X_train_stack,Y_train_stack)\n",
    "predict_proba=mnb_train.predict_proba(X_sub_stack)\n",
    "df_tfidf_sub_stack=pd.DataFrame(predict_proba[:,[0,1]],columns=['tfidf_mnb_feat1','tfidf_mnb_feat2'])\n",
    "df_tfidf_sub_stack['tfidf_mnb_feat1']=df_tfidf_sub_stack['tfidf_mnb_feat1'].apply(lambda x :np.round(x,2))\n",
    "df_tfidf_sub_stack['tfidf_mnb_feat2']=df_tfidf_sub_stack['tfidf_mnb_feat2'].apply(lambda x :np.round(x,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cvt features for submission data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cvt_Vectorizer=CountVectorizer(stop_words='english')\n",
    "Xcvt_train_stack=Cvt_Vectorizer.fit_transform(df['text'].get_values())\n",
    "Ycvt_train_stack=df['Target'].get_values()\n",
    "Xcvt_sub_stack=Cvt_Vectorizer.transform(df_sub['text'].get_values())\n",
    "mnbcvt_train=MultinomialNB(alpha=1)\n",
    "mnbcvt_train.fit(Xcvt_train_stack,Ycvt_train_stack)\n",
    "cvt_predict_proba=mnbcvt_train.predict_proba(Xcvt_sub_stack)\n",
    "df_cvt_sub_stack=pd.DataFrame(cvt_predict_proba[:,[0,1]],columns=['cvt_mnb_feat1','cvt_mnb_feat2'])\n",
    "df_cvt_sub_stack['cvt_mnb_feat1']=df_cvt_sub_stack['cvt_mnb_feat1'].apply(lambda x :np.round(x,2))\n",
    "df_cvt_sub_stack['cvt_mnb_feat2']=df_cvt_sub_stack['cvt_mnb_feat2'].apply(lambda x :np.round(x,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov features for submission **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "dataframe_sub=Conditional_n_grams(list(df['text'].get_values()),list(df['author'].get_values()),4)\n",
    "predict_proba_mk_sub=Target_Prediction(df_sub['text'].get_values(),4,prior=dataframe_sub,alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_mk=pd.DataFrame(predict_proba_mk_sub[:,[0,1]],columns=['mk_sub_feat1','mk_sub_feat2'])\n",
    "df_sub_mk['mk_sub_feat1']=df_sub_mk['mk_sub_feat1'].apply(lambda x : np.round(x,2))\n",
    "df_sub_mk['mk_sub_feat2']=df_sub_mk['mk_sub_feat2'].apply(lambda x : np.round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_stacked=pd.concat([df_tfidf_sub_stack,df_cvt_sub_stack,df_sub_mk],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_mnb_feat1</th>\n",
       "      <th>tfidf_mnb_feat2</th>\n",
       "      <th>cvt_mnb_feat1</th>\n",
       "      <th>cvt_mnb_feat2</th>\n",
       "      <th>mk_sub_feat1</th>\n",
       "      <th>mk_sub_feat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tfidf_mnb_feat1  tfidf_mnb_feat2  cvt_mnb_feat1  cvt_mnb_feat2  \\\n",
       "0             0.04             0.01           0.00           0.00   \n",
       "1             0.97             0.01           1.00           0.00   \n",
       "2             0.60             0.40           0.18           0.82   \n",
       "3             0.66             0.34           0.45           0.55   \n",
       "4             0.86             0.10           0.97           0.02   \n",
       "5             0.80             0.16           0.86           0.14   \n",
       "6             0.48             0.36           0.50           0.46   \n",
       "7             0.08             0.17           0.00           0.04   \n",
       "8             0.99             0.01           1.00           0.00   \n",
       "9             0.67             0.19           0.80           0.07   \n",
       "\n",
       "   mk_sub_feat1  mk_sub_feat2  \n",
       "0          0.00          0.00  \n",
       "1          1.00          0.00  \n",
       "2          0.00          1.00  \n",
       "3          0.09          0.91  \n",
       "4          0.97          0.00  \n",
       "5          1.00          0.00  \n",
       "6          0.99          0.01  \n",
       "7          0.00          0.00  \n",
       "8          1.00          0.00  \n",
       "9          0.99          0.01  "
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_stacked.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Meta features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['n_words']=df_sub['text'].apply(lambda x : len(x.split(\" \")))\n",
    "df_sub['Period']=df_sub['text'].str.count('\\.')\n",
    "df_sub['Comma']=df_sub['text'].str.count(',')\n",
    "df_sub['Question_Marks']=df_sub['text'].str.count(r'\\?')\n",
    "df_sub['Colon']=df_sub['text'].str.count(r':')\n",
    "df_sub['Semicolon']=df_sub['text'].str.count(';')\n",
    "df_sub['Quotation_Mark']=df_sub['text'].str.count('\"')\n",
    "df_sub['Aphostrophe']=df_sub['text'].str.count(\"'\")\n",
    "number_of_title_words=[]\n",
    "for sentence in df_sub['text']:\n",
    "    doc=nlp(sentence)\n",
    "    p=0\n",
    "    for tokens in doc:\n",
    "        if tokens.text.istitle() :\n",
    "            p=p+1\n",
    "    number_of_title_words.append(p)  \n",
    "df_sub['Title_Case_Words']=number_of_title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_meta=df_sub[['n_words','Period','Comma','Question_Marks','Colon','Semicolon','Quotation_Mark','Aphostrophe','Title_Case_Words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSI features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub_Tfidf=Vectorizer_stack.transform(df_sub['text'].get_values())\n",
    "X_sub_svd=svd.transform(X_sub_Tfidf)\n",
    "svd_sub_columns=[]\n",
    "for p in np.arange(30):\n",
    "    svd_sub_columns.append('svd_sub_columns_'+str(p))\n",
    "df_sub_svd=pd.DataFrame(X_sub_svd,columns=svd_sub_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_stacked=pd.concat([df_sub_meta,df_sub_svd,df_sub_stacked],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8392, 45)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Question_Marks</th>\n",
       "      <th>Colon</th>\n",
       "      <th>Semicolon</th>\n",
       "      <th>Quotation_Mark</th>\n",
       "      <th>Aphostrophe</th>\n",
       "      <th>Title_Case_Words</th>\n",
       "      <th>svd_columns_0</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_columns_26</th>\n",
       "      <th>svd_columns_27</th>\n",
       "      <th>svd_columns_28</th>\n",
       "      <th>svd_columns_29</th>\n",
       "      <th>Tfidf_feat1</th>\n",
       "      <th>Tfidf_feat2</th>\n",
       "      <th>cvtNb_feat1</th>\n",
       "      <th>cvtNb_feat2</th>\n",
       "      <th>mk_feat1</th>\n",
       "      <th>mk_feat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.061037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007574</td>\n",
       "      <td>-0.021656</td>\n",
       "      <td>-0.000622</td>\n",
       "      <td>-0.053015</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.007604</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>-0.014414</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080321</td>\n",
       "      <td>-0.009206</td>\n",
       "      <td>-0.043403</td>\n",
       "      <td>0.060264</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.068588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043884</td>\n",
       "      <td>0.025721</td>\n",
       "      <td>-0.050824</td>\n",
       "      <td>-0.034901</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.083893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095226</td>\n",
       "      <td>0.047396</td>\n",
       "      <td>-0.033463</td>\n",
       "      <td>-0.023350</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028070</td>\n",
       "      <td>-0.011223</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>-0.046288</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014893</td>\n",
       "      <td>-0.003963</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>-0.014358</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.118808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>0.046941</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>0.041659</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>-0.002347</td>\n",
       "      <td>-0.005261</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_words  Period  Comma  Question_Marks  Colon  Semicolon  Quotation_Mark  \\\n",
       "0       41       1      4               0      0          2               0   \n",
       "1       14       1      0               0      0          0               0   \n",
       "2       36       1      4               0      0          0               0   \n",
       "3       34       1      3               0      0          0               0   \n",
       "4       27       1      2               0      0          1               0   \n",
       "5       83       1      4               0      1          0               0   \n",
       "6       21       1      3               0      0          1               0   \n",
       "7        8       1      0               0      0          0               0   \n",
       "8       88       1      7               0      0          1               0   \n",
       "9       23       1      2               0      0          0               0   \n",
       "\n",
       "   Aphostrophe  Title_Case_Words  svd_columns_0    ...     svd_columns_26  \\\n",
       "0            0                 3       0.061037    ...          -0.007574   \n",
       "1            0                 1       0.019324    ...           0.000005   \n",
       "2            0                 1       0.066671    ...          -0.080321   \n",
       "3            0                 4       0.068588    ...          -0.043884   \n",
       "4            0                 2       0.028288    ...          -0.000827   \n",
       "5            0                 5       0.083893    ...          -0.095226   \n",
       "6            0                 1       0.050682    ...           0.028070   \n",
       "7            0                 1       0.019461    ...          -0.014893   \n",
       "8            2                 7       0.118808    ...           0.007728   \n",
       "9            0                 1       0.013881    ...           0.000365   \n",
       "\n",
       "   svd_columns_27  svd_columns_28  svd_columns_29  Tfidf_feat1  Tfidf_feat2  \\\n",
       "0       -0.021656       -0.000622       -0.053015         0.99         0.00   \n",
       "1       -0.007604        0.012291       -0.014414         0.20         0.76   \n",
       "2       -0.009206       -0.043403        0.060264         0.96         0.04   \n",
       "3        0.025721       -0.050824       -0.034901         0.00         0.00   \n",
       "4        0.000556        0.006208        0.002744         0.61         0.36   \n",
       "5        0.047396       -0.033463       -0.023350         0.00         0.00   \n",
       "6       -0.011223       -0.014391       -0.046288         0.79         0.14   \n",
       "7       -0.003963        0.002818       -0.014358         0.98         0.01   \n",
       "8        0.046941       -0.000738        0.041659         0.98         0.02   \n",
       "9        0.002515       -0.002347       -0.005261         0.29         0.03   \n",
       "\n",
       "   cvtNb_feat1  cvtNb_feat2  mk_feat1  mk_feat2  \n",
       "0         1.00         0.00      1.00      0.00  \n",
       "1         0.88         0.09      0.00      1.00  \n",
       "2         0.98         0.02      1.00      0.00  \n",
       "3         0.00         0.00      0.00      0.00  \n",
       "4         0.63         0.37      0.53      0.46  \n",
       "5         0.00         0.00      0.00      0.00  \n",
       "6         0.99         0.01      1.00      0.00  \n",
       "7         0.93         0.04      0.81      0.10  \n",
       "8         1.00         0.00      1.00      0.00  \n",
       "9         0.16         0.02      0.00      0.00  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Question_Marks</th>\n",
       "      <th>Colon</th>\n",
       "      <th>Semicolon</th>\n",
       "      <th>Quotation_Mark</th>\n",
       "      <th>Aphostrophe</th>\n",
       "      <th>Title_Case_Words</th>\n",
       "      <th>svd_sub_columns_0</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_sub_columns_26</th>\n",
       "      <th>svd_sub_columns_27</th>\n",
       "      <th>svd_sub_columns_28</th>\n",
       "      <th>svd_sub_columns_29</th>\n",
       "      <th>tfidf_mnb_feat1</th>\n",
       "      <th>tfidf_mnb_feat2</th>\n",
       "      <th>cvt_mnb_feat1</th>\n",
       "      <th>cvt_mnb_feat2</th>\n",
       "      <th>mk_sub_feat1</th>\n",
       "      <th>mk_sub_feat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.055677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031527</td>\n",
       "      <td>-0.025241</td>\n",
       "      <td>-0.062803</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002258</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>-0.010056</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018751</td>\n",
       "      <td>0.022615</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>-0.001938</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.029997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>-0.005152</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>-0.010117</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007936</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.009083</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010588</td>\n",
       "      <td>-0.000419</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>-0.008716</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036670</td>\n",
       "      <td>-0.040631</td>\n",
       "      <td>0.143580</td>\n",
       "      <td>0.017636</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.143744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056955</td>\n",
       "      <td>0.032518</td>\n",
       "      <td>0.018353</td>\n",
       "      <td>-0.012188</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.032659</td>\n",
       "      <td>0.076846</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>0.039765</td>\n",
       "      <td>-0.094125</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_words  Period  Comma  Question_Marks  Colon  Semicolon  Quotation_Mark  \\\n",
       "0       19       1      2               0      0          0               0   \n",
       "1       62       1      6               0      0          0               0   \n",
       "2       33       1      1               0      1          0               0   \n",
       "3       41       1      4               0      0          0               0   \n",
       "4       11       1      0               0      0          0               0   \n",
       "5       32       1      6               0      0          0               1   \n",
       "6       13       1      1               0      0          0               0   \n",
       "7       30       1      1               0      0          1               0   \n",
       "8       28       1      4               0      0          0               0   \n",
       "9        8       1      0               0      0          0               1   \n",
       "\n",
       "   Aphostrophe  Title_Case_Words  svd_sub_columns_0      ...       \\\n",
       "0            0                 3           0.055677      ...        \n",
       "1            0                 3           0.054519      ...        \n",
       "2            0                 1           0.039503      ...        \n",
       "3            0                 3           0.029997      ...        \n",
       "4            0                 1           0.023256      ...        \n",
       "5            0                 3           0.034411      ...        \n",
       "6            0                 1           0.056038      ...        \n",
       "7            0                 5           0.143744      ...        \n",
       "8            0                 1           0.084013      ...        \n",
       "9            0                 1           0.033882      ...        \n",
       "\n",
       "   svd_sub_columns_26  svd_sub_columns_27  svd_sub_columns_28  \\\n",
       "0            0.031527           -0.025241           -0.062803   \n",
       "1           -0.002258            0.007305            0.008341   \n",
       "2           -0.018751            0.022615            0.003829   \n",
       "3            0.005526           -0.005152            0.000271   \n",
       "4           -0.007936            0.000834           -0.002123   \n",
       "5           -0.010588           -0.000419            0.015033   \n",
       "6           -0.036670           -0.040631            0.143580   \n",
       "7           -0.056955            0.032518            0.018353   \n",
       "8            0.078137            0.032659            0.076846   \n",
       "9            0.027890           -0.039498            0.039765   \n",
       "\n",
       "   svd_sub_columns_29  tfidf_mnb_feat1  tfidf_mnb_feat2  cvt_mnb_feat1  \\\n",
       "0            0.033175             0.04             0.01           0.00   \n",
       "1           -0.010056             0.97             0.01           1.00   \n",
       "2           -0.001938             0.60             0.40           0.18   \n",
       "3           -0.010117             0.66             0.34           0.45   \n",
       "4           -0.009083             0.86             0.10           0.97   \n",
       "5           -0.008716             0.80             0.16           0.86   \n",
       "6            0.017636             0.48             0.36           0.50   \n",
       "7           -0.012188             0.08             0.17           0.00   \n",
       "8            0.014953             0.99             0.01           1.00   \n",
       "9           -0.094125             0.67             0.19           0.80   \n",
       "\n",
       "   cvt_mnb_feat2  mk_sub_feat1  mk_sub_feat2  \n",
       "0           0.00          0.00          0.00  \n",
       "1           0.00          1.00          0.00  \n",
       "2           0.82          0.00          1.00  \n",
       "3           0.55          0.09          0.91  \n",
       "4           0.02          0.97          0.00  \n",
       "5           0.14          1.00          0.00  \n",
       "6           0.46          0.99          0.01  \n",
       "7           0.04          0.00          0.00  \n",
       "8           0.00          1.00          0.00  \n",
       "9           0.07          0.99          0.01  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_stacked.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Traning XGboost on stacked data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB=XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=500,silent=False,objective='multi:softmax',sub_sample=0.8,colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.00537\tvalidation_1-mlogloss:1.00712\n",
      "Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-mlogloss hasn't improved in 50 rounds.\n",
      "[1]\tvalidation_0-mlogloss:0.924996\tvalidation_1-mlogloss:0.928346\n",
      "[2]\tvalidation_0-mlogloss:0.856462\tvalidation_1-mlogloss:0.8616\n",
      "[3]\tvalidation_0-mlogloss:0.798981\tvalidation_1-mlogloss:0.805521\n",
      "[4]\tvalidation_0-mlogloss:0.748317\tvalidation_1-mlogloss:0.756266\n",
      "[5]\tvalidation_0-mlogloss:0.704288\tvalidation_1-mlogloss:0.713512\n",
      "[6]\tvalidation_0-mlogloss:0.66591\tvalidation_1-mlogloss:0.676311\n",
      "[7]\tvalidation_0-mlogloss:0.632479\tvalidation_1-mlogloss:0.644148\n",
      "[8]\tvalidation_0-mlogloss:0.604222\tvalidation_1-mlogloss:0.616872\n",
      "[9]\tvalidation_0-mlogloss:0.578154\tvalidation_1-mlogloss:0.591766\n",
      "[10]\tvalidation_0-mlogloss:0.55533\tvalidation_1-mlogloss:0.569854\n",
      "[11]\tvalidation_0-mlogloss:0.534787\tvalidation_1-mlogloss:0.550178\n",
      "[12]\tvalidation_0-mlogloss:0.516758\tvalidation_1-mlogloss:0.532865\n",
      "[13]\tvalidation_0-mlogloss:0.500467\tvalidation_1-mlogloss:0.517261\n",
      "[14]\tvalidation_0-mlogloss:0.486026\tvalidation_1-mlogloss:0.503498\n",
      "[15]\tvalidation_0-mlogloss:0.473024\tvalidation_1-mlogloss:0.49107\n",
      "[16]\tvalidation_0-mlogloss:0.46115\tvalidation_1-mlogloss:0.479923\n",
      "[17]\tvalidation_0-mlogloss:0.450453\tvalidation_1-mlogloss:0.469808\n",
      "[18]\tvalidation_0-mlogloss:0.44077\tvalidation_1-mlogloss:0.460463\n",
      "[19]\tvalidation_0-mlogloss:0.432515\tvalidation_1-mlogloss:0.452763\n",
      "[20]\tvalidation_0-mlogloss:0.424933\tvalidation_1-mlogloss:0.445691\n",
      "[21]\tvalidation_0-mlogloss:0.417822\tvalidation_1-mlogloss:0.439141\n",
      "[22]\tvalidation_0-mlogloss:0.411463\tvalidation_1-mlogloss:0.433571\n",
      "[23]\tvalidation_0-mlogloss:0.405832\tvalidation_1-mlogloss:0.42827\n",
      "[24]\tvalidation_0-mlogloss:0.400733\tvalidation_1-mlogloss:0.423651\n",
      "[25]\tvalidation_0-mlogloss:0.39577\tvalidation_1-mlogloss:0.419224\n",
      "[26]\tvalidation_0-mlogloss:0.391419\tvalidation_1-mlogloss:0.415438\n",
      "[27]\tvalidation_0-mlogloss:0.38733\tvalidation_1-mlogloss:0.411812\n",
      "[28]\tvalidation_0-mlogloss:0.383626\tvalidation_1-mlogloss:0.408847\n",
      "[29]\tvalidation_0-mlogloss:0.38034\tvalidation_1-mlogloss:0.40572\n",
      "[30]\tvalidation_0-mlogloss:0.377348\tvalidation_1-mlogloss:0.402955\n",
      "[31]\tvalidation_0-mlogloss:0.374456\tvalidation_1-mlogloss:0.400605\n",
      "[32]\tvalidation_0-mlogloss:0.371833\tvalidation_1-mlogloss:0.398481\n",
      "[33]\tvalidation_0-mlogloss:0.369367\tvalidation_1-mlogloss:0.396539\n",
      "[34]\tvalidation_0-mlogloss:0.367079\tvalidation_1-mlogloss:0.394672\n",
      "[35]\tvalidation_0-mlogloss:0.365034\tvalidation_1-mlogloss:0.393113\n",
      "[36]\tvalidation_0-mlogloss:0.363153\tvalidation_1-mlogloss:0.391673\n",
      "[37]\tvalidation_0-mlogloss:0.361402\tvalidation_1-mlogloss:0.390281\n",
      "[38]\tvalidation_0-mlogloss:0.359624\tvalidation_1-mlogloss:0.388952\n",
      "[39]\tvalidation_0-mlogloss:0.357902\tvalidation_1-mlogloss:0.387678\n",
      "[40]\tvalidation_0-mlogloss:0.356359\tvalidation_1-mlogloss:0.386574\n",
      "[41]\tvalidation_0-mlogloss:0.354815\tvalidation_1-mlogloss:0.385402\n",
      "[42]\tvalidation_0-mlogloss:0.353108\tvalidation_1-mlogloss:0.38404\n",
      "[43]\tvalidation_0-mlogloss:0.351806\tvalidation_1-mlogloss:0.383213\n",
      "[44]\tvalidation_0-mlogloss:0.35043\tvalidation_1-mlogloss:0.382455\n",
      "[45]\tvalidation_0-mlogloss:0.349043\tvalidation_1-mlogloss:0.381401\n",
      "[46]\tvalidation_0-mlogloss:0.34784\tvalidation_1-mlogloss:0.380729\n",
      "[47]\tvalidation_0-mlogloss:0.346697\tvalidation_1-mlogloss:0.379998\n",
      "[48]\tvalidation_0-mlogloss:0.345543\tvalidation_1-mlogloss:0.379288\n",
      "[49]\tvalidation_0-mlogloss:0.344427\tvalidation_1-mlogloss:0.378567\n",
      "[50]\tvalidation_0-mlogloss:0.343453\tvalidation_1-mlogloss:0.377902\n",
      "[51]\tvalidation_0-mlogloss:0.342408\tvalidation_1-mlogloss:0.377511\n",
      "[52]\tvalidation_0-mlogloss:0.341636\tvalidation_1-mlogloss:0.377002\n",
      "[53]\tvalidation_0-mlogloss:0.340457\tvalidation_1-mlogloss:0.376311\n",
      "[54]\tvalidation_0-mlogloss:0.339473\tvalidation_1-mlogloss:0.375571\n",
      "[55]\tvalidation_0-mlogloss:0.33861\tvalidation_1-mlogloss:0.375232\n",
      "[56]\tvalidation_0-mlogloss:0.337576\tvalidation_1-mlogloss:0.37459\n",
      "[57]\tvalidation_0-mlogloss:0.336634\tvalidation_1-mlogloss:0.374239\n",
      "[58]\tvalidation_0-mlogloss:0.335744\tvalidation_1-mlogloss:0.373775\n",
      "[59]\tvalidation_0-mlogloss:0.334845\tvalidation_1-mlogloss:0.373352\n",
      "[60]\tvalidation_0-mlogloss:0.333881\tvalidation_1-mlogloss:0.372931\n",
      "[61]\tvalidation_0-mlogloss:0.332948\tvalidation_1-mlogloss:0.372608\n",
      "[62]\tvalidation_0-mlogloss:0.332192\tvalidation_1-mlogloss:0.372445\n",
      "[63]\tvalidation_0-mlogloss:0.331408\tvalidation_1-mlogloss:0.372386\n",
      "[64]\tvalidation_0-mlogloss:0.330601\tvalidation_1-mlogloss:0.371887\n",
      "[65]\tvalidation_0-mlogloss:0.329885\tvalidation_1-mlogloss:0.371642\n",
      "[66]\tvalidation_0-mlogloss:0.329113\tvalidation_1-mlogloss:0.371287\n",
      "[67]\tvalidation_0-mlogloss:0.328363\tvalidation_1-mlogloss:0.370794\n",
      "[68]\tvalidation_0-mlogloss:0.327576\tvalidation_1-mlogloss:0.370635\n",
      "[69]\tvalidation_0-mlogloss:0.326972\tvalidation_1-mlogloss:0.370331\n",
      "[70]\tvalidation_0-mlogloss:0.326274\tvalidation_1-mlogloss:0.369868\n",
      "[71]\tvalidation_0-mlogloss:0.325485\tvalidation_1-mlogloss:0.369632\n",
      "[72]\tvalidation_0-mlogloss:0.324851\tvalidation_1-mlogloss:0.369352\n",
      "[73]\tvalidation_0-mlogloss:0.324123\tvalidation_1-mlogloss:0.369166\n",
      "[74]\tvalidation_0-mlogloss:0.323463\tvalidation_1-mlogloss:0.368868\n",
      "[75]\tvalidation_0-mlogloss:0.32282\tvalidation_1-mlogloss:0.368729\n",
      "[76]\tvalidation_0-mlogloss:0.322201\tvalidation_1-mlogloss:0.36843\n",
      "[77]\tvalidation_0-mlogloss:0.321543\tvalidation_1-mlogloss:0.36827\n",
      "[78]\tvalidation_0-mlogloss:0.320921\tvalidation_1-mlogloss:0.368011\n",
      "[79]\tvalidation_0-mlogloss:0.320302\tvalidation_1-mlogloss:0.367782\n",
      "[80]\tvalidation_0-mlogloss:0.319712\tvalidation_1-mlogloss:0.367528\n",
      "[81]\tvalidation_0-mlogloss:0.319144\tvalidation_1-mlogloss:0.367425\n",
      "[82]\tvalidation_0-mlogloss:0.318653\tvalidation_1-mlogloss:0.36712\n",
      "[83]\tvalidation_0-mlogloss:0.318037\tvalidation_1-mlogloss:0.367024\n",
      "[84]\tvalidation_0-mlogloss:0.317627\tvalidation_1-mlogloss:0.366935\n",
      "[85]\tvalidation_0-mlogloss:0.317032\tvalidation_1-mlogloss:0.366748\n",
      "[86]\tvalidation_0-mlogloss:0.316518\tvalidation_1-mlogloss:0.36671\n",
      "[87]\tvalidation_0-mlogloss:0.316026\tvalidation_1-mlogloss:0.366506\n",
      "[88]\tvalidation_0-mlogloss:0.315604\tvalidation_1-mlogloss:0.366429\n",
      "[89]\tvalidation_0-mlogloss:0.315123\tvalidation_1-mlogloss:0.366259\n",
      "[90]\tvalidation_0-mlogloss:0.314542\tvalidation_1-mlogloss:0.365981\n",
      "[91]\tvalidation_0-mlogloss:0.314026\tvalidation_1-mlogloss:0.365884\n",
      "[92]\tvalidation_0-mlogloss:0.313516\tvalidation_1-mlogloss:0.365649\n",
      "[93]\tvalidation_0-mlogloss:0.31313\tvalidation_1-mlogloss:0.365412\n",
      "[94]\tvalidation_0-mlogloss:0.312524\tvalidation_1-mlogloss:0.365347\n",
      "[95]\tvalidation_0-mlogloss:0.311918\tvalidation_1-mlogloss:0.365129\n",
      "[96]\tvalidation_0-mlogloss:0.311404\tvalidation_1-mlogloss:0.365181\n",
      "[97]\tvalidation_0-mlogloss:0.310866\tvalidation_1-mlogloss:0.365155\n",
      "[98]\tvalidation_0-mlogloss:0.310553\tvalidation_1-mlogloss:0.365098\n",
      "[99]\tvalidation_0-mlogloss:0.310089\tvalidation_1-mlogloss:0.36506\n",
      "[100]\tvalidation_0-mlogloss:0.309659\tvalidation_1-mlogloss:0.364846\n",
      "[101]\tvalidation_0-mlogloss:0.309341\tvalidation_1-mlogloss:0.364874\n",
      "[102]\tvalidation_0-mlogloss:0.308826\tvalidation_1-mlogloss:0.364744\n",
      "[103]\tvalidation_0-mlogloss:0.308341\tvalidation_1-mlogloss:0.364542\n",
      "[104]\tvalidation_0-mlogloss:0.30781\tvalidation_1-mlogloss:0.364456\n",
      "[105]\tvalidation_0-mlogloss:0.307279\tvalidation_1-mlogloss:0.364271\n",
      "[106]\tvalidation_0-mlogloss:0.306877\tvalidation_1-mlogloss:0.364164\n",
      "[107]\tvalidation_0-mlogloss:0.306348\tvalidation_1-mlogloss:0.363984\n",
      "[108]\tvalidation_0-mlogloss:0.305988\tvalidation_1-mlogloss:0.363972\n",
      "[109]\tvalidation_0-mlogloss:0.30549\tvalidation_1-mlogloss:0.364016\n",
      "[110]\tvalidation_0-mlogloss:0.305089\tvalidation_1-mlogloss:0.363968\n",
      "[111]\tvalidation_0-mlogloss:0.304594\tvalidation_1-mlogloss:0.363974\n",
      "[112]\tvalidation_0-mlogloss:0.304178\tvalidation_1-mlogloss:0.363921\n",
      "[113]\tvalidation_0-mlogloss:0.30372\tvalidation_1-mlogloss:0.363803\n",
      "[114]\tvalidation_0-mlogloss:0.303232\tvalidation_1-mlogloss:0.363687\n",
      "[115]\tvalidation_0-mlogloss:0.302902\tvalidation_1-mlogloss:0.36357\n",
      "[116]\tvalidation_0-mlogloss:0.302394\tvalidation_1-mlogloss:0.363505\n",
      "[117]\tvalidation_0-mlogloss:0.302013\tvalidation_1-mlogloss:0.363442\n",
      "[118]\tvalidation_0-mlogloss:0.301506\tvalidation_1-mlogloss:0.363486\n",
      "[119]\tvalidation_0-mlogloss:0.301111\tvalidation_1-mlogloss:0.363389\n",
      "[120]\tvalidation_0-mlogloss:0.300729\tvalidation_1-mlogloss:0.36321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121]\tvalidation_0-mlogloss:0.300394\tvalidation_1-mlogloss:0.363056\n",
      "[122]\tvalidation_0-mlogloss:0.299922\tvalidation_1-mlogloss:0.362995\n",
      "[123]\tvalidation_0-mlogloss:0.299486\tvalidation_1-mlogloss:0.362836\n",
      "[124]\tvalidation_0-mlogloss:0.299098\tvalidation_1-mlogloss:0.362785\n",
      "[125]\tvalidation_0-mlogloss:0.298656\tvalidation_1-mlogloss:0.362791\n",
      "[126]\tvalidation_0-mlogloss:0.298252\tvalidation_1-mlogloss:0.362638\n",
      "[127]\tvalidation_0-mlogloss:0.297856\tvalidation_1-mlogloss:0.362531\n",
      "[128]\tvalidation_0-mlogloss:0.297442\tvalidation_1-mlogloss:0.362409\n",
      "[129]\tvalidation_0-mlogloss:0.2971\tvalidation_1-mlogloss:0.362356\n",
      "[130]\tvalidation_0-mlogloss:0.296804\tvalidation_1-mlogloss:0.362339\n",
      "[131]\tvalidation_0-mlogloss:0.296481\tvalidation_1-mlogloss:0.362332\n",
      "[132]\tvalidation_0-mlogloss:0.296008\tvalidation_1-mlogloss:0.362376\n",
      "[133]\tvalidation_0-mlogloss:0.295689\tvalidation_1-mlogloss:0.362326\n",
      "[134]\tvalidation_0-mlogloss:0.295152\tvalidation_1-mlogloss:0.362241\n",
      "[135]\tvalidation_0-mlogloss:0.29474\tvalidation_1-mlogloss:0.362082\n",
      "[136]\tvalidation_0-mlogloss:0.294309\tvalidation_1-mlogloss:0.362065\n",
      "[137]\tvalidation_0-mlogloss:0.293992\tvalidation_1-mlogloss:0.361962\n",
      "[138]\tvalidation_0-mlogloss:0.293629\tvalidation_1-mlogloss:0.361946\n",
      "[139]\tvalidation_0-mlogloss:0.293373\tvalidation_1-mlogloss:0.361875\n",
      "[140]\tvalidation_0-mlogloss:0.293092\tvalidation_1-mlogloss:0.361852\n",
      "[141]\tvalidation_0-mlogloss:0.292817\tvalidation_1-mlogloss:0.361865\n",
      "[142]\tvalidation_0-mlogloss:0.292383\tvalidation_1-mlogloss:0.361865\n",
      "[143]\tvalidation_0-mlogloss:0.292073\tvalidation_1-mlogloss:0.361787\n",
      "[144]\tvalidation_0-mlogloss:0.291715\tvalidation_1-mlogloss:0.361773\n",
      "[145]\tvalidation_0-mlogloss:0.291376\tvalidation_1-mlogloss:0.36169\n",
      "[146]\tvalidation_0-mlogloss:0.291101\tvalidation_1-mlogloss:0.361621\n",
      "[147]\tvalidation_0-mlogloss:0.290862\tvalidation_1-mlogloss:0.361588\n",
      "[148]\tvalidation_0-mlogloss:0.290494\tvalidation_1-mlogloss:0.361494\n",
      "[149]\tvalidation_0-mlogloss:0.290226\tvalidation_1-mlogloss:0.361508\n",
      "[150]\tvalidation_0-mlogloss:0.289853\tvalidation_1-mlogloss:0.361503\n",
      "[151]\tvalidation_0-mlogloss:0.289561\tvalidation_1-mlogloss:0.361457\n",
      "[152]\tvalidation_0-mlogloss:0.28923\tvalidation_1-mlogloss:0.361505\n",
      "[153]\tvalidation_0-mlogloss:0.288846\tvalidation_1-mlogloss:0.36137\n",
      "[154]\tvalidation_0-mlogloss:0.288454\tvalidation_1-mlogloss:0.36133\n",
      "[155]\tvalidation_0-mlogloss:0.288141\tvalidation_1-mlogloss:0.361369\n",
      "[156]\tvalidation_0-mlogloss:0.287825\tvalidation_1-mlogloss:0.361396\n",
      "[157]\tvalidation_0-mlogloss:0.287481\tvalidation_1-mlogloss:0.361273\n",
      "[158]\tvalidation_0-mlogloss:0.287113\tvalidation_1-mlogloss:0.361238\n",
      "[159]\tvalidation_0-mlogloss:0.286736\tvalidation_1-mlogloss:0.361256\n",
      "[160]\tvalidation_0-mlogloss:0.286513\tvalidation_1-mlogloss:0.361205\n",
      "[161]\tvalidation_0-mlogloss:0.286286\tvalidation_1-mlogloss:0.361156\n",
      "[162]\tvalidation_0-mlogloss:0.286011\tvalidation_1-mlogloss:0.361095\n",
      "[163]\tvalidation_0-mlogloss:0.285725\tvalidation_1-mlogloss:0.361024\n",
      "[164]\tvalidation_0-mlogloss:0.285386\tvalidation_1-mlogloss:0.360952\n",
      "[165]\tvalidation_0-mlogloss:0.285092\tvalidation_1-mlogloss:0.360971\n",
      "[166]\tvalidation_0-mlogloss:0.284719\tvalidation_1-mlogloss:0.360888\n",
      "[167]\tvalidation_0-mlogloss:0.284332\tvalidation_1-mlogloss:0.360803\n",
      "[168]\tvalidation_0-mlogloss:0.284012\tvalidation_1-mlogloss:0.3609\n",
      "[169]\tvalidation_0-mlogloss:0.283653\tvalidation_1-mlogloss:0.36086\n",
      "[170]\tvalidation_0-mlogloss:0.283334\tvalidation_1-mlogloss:0.360866\n",
      "[171]\tvalidation_0-mlogloss:0.282968\tvalidation_1-mlogloss:0.360966\n",
      "[172]\tvalidation_0-mlogloss:0.282672\tvalidation_1-mlogloss:0.360997\n",
      "[173]\tvalidation_0-mlogloss:0.282398\tvalidation_1-mlogloss:0.360995\n",
      "[174]\tvalidation_0-mlogloss:0.282046\tvalidation_1-mlogloss:0.360943\n",
      "[175]\tvalidation_0-mlogloss:0.281757\tvalidation_1-mlogloss:0.360877\n",
      "[176]\tvalidation_0-mlogloss:0.281437\tvalidation_1-mlogloss:0.360816\n",
      "[177]\tvalidation_0-mlogloss:0.28118\tvalidation_1-mlogloss:0.360841\n",
      "[178]\tvalidation_0-mlogloss:0.280901\tvalidation_1-mlogloss:0.36089\n",
      "[179]\tvalidation_0-mlogloss:0.280535\tvalidation_1-mlogloss:0.360875\n",
      "[180]\tvalidation_0-mlogloss:0.280211\tvalidation_1-mlogloss:0.360793\n",
      "[181]\tvalidation_0-mlogloss:0.279812\tvalidation_1-mlogloss:0.360756\n",
      "[182]\tvalidation_0-mlogloss:0.279433\tvalidation_1-mlogloss:0.36078\n",
      "[183]\tvalidation_0-mlogloss:0.279043\tvalidation_1-mlogloss:0.360785\n",
      "[184]\tvalidation_0-mlogloss:0.278796\tvalidation_1-mlogloss:0.360722\n",
      "[185]\tvalidation_0-mlogloss:0.278507\tvalidation_1-mlogloss:0.360768\n",
      "[186]\tvalidation_0-mlogloss:0.27833\tvalidation_1-mlogloss:0.36072\n",
      "[187]\tvalidation_0-mlogloss:0.278002\tvalidation_1-mlogloss:0.360679\n",
      "[188]\tvalidation_0-mlogloss:0.277792\tvalidation_1-mlogloss:0.360713\n",
      "[189]\tvalidation_0-mlogloss:0.277444\tvalidation_1-mlogloss:0.360664\n",
      "[190]\tvalidation_0-mlogloss:0.277103\tvalidation_1-mlogloss:0.360666\n",
      "[191]\tvalidation_0-mlogloss:0.276774\tvalidation_1-mlogloss:0.36056\n",
      "[192]\tvalidation_0-mlogloss:0.2764\tvalidation_1-mlogloss:0.360512\n",
      "[193]\tvalidation_0-mlogloss:0.276149\tvalidation_1-mlogloss:0.360444\n",
      "[194]\tvalidation_0-mlogloss:0.275846\tvalidation_1-mlogloss:0.36045\n",
      "[195]\tvalidation_0-mlogloss:0.27561\tvalidation_1-mlogloss:0.360518\n",
      "[196]\tvalidation_0-mlogloss:0.275325\tvalidation_1-mlogloss:0.360491\n",
      "[197]\tvalidation_0-mlogloss:0.274944\tvalidation_1-mlogloss:0.360467\n",
      "[198]\tvalidation_0-mlogloss:0.274572\tvalidation_1-mlogloss:0.360416\n",
      "[199]\tvalidation_0-mlogloss:0.274299\tvalidation_1-mlogloss:0.360383\n",
      "[200]\tvalidation_0-mlogloss:0.274\tvalidation_1-mlogloss:0.360405\n",
      "[201]\tvalidation_0-mlogloss:0.273749\tvalidation_1-mlogloss:0.360433\n",
      "[202]\tvalidation_0-mlogloss:0.27346\tvalidation_1-mlogloss:0.360468\n",
      "[203]\tvalidation_0-mlogloss:0.273185\tvalidation_1-mlogloss:0.360481\n",
      "[204]\tvalidation_0-mlogloss:0.27296\tvalidation_1-mlogloss:0.360549\n",
      "[205]\tvalidation_0-mlogloss:0.272722\tvalidation_1-mlogloss:0.360633\n",
      "[206]\tvalidation_0-mlogloss:0.27245\tvalidation_1-mlogloss:0.360671\n",
      "[207]\tvalidation_0-mlogloss:0.272166\tvalidation_1-mlogloss:0.36067\n",
      "[208]\tvalidation_0-mlogloss:0.271844\tvalidation_1-mlogloss:0.360676\n",
      "[209]\tvalidation_0-mlogloss:0.271526\tvalidation_1-mlogloss:0.360687\n",
      "[210]\tvalidation_0-mlogloss:0.271248\tvalidation_1-mlogloss:0.360578\n",
      "[211]\tvalidation_0-mlogloss:0.270906\tvalidation_1-mlogloss:0.36054\n",
      "[212]\tvalidation_0-mlogloss:0.270616\tvalidation_1-mlogloss:0.360567\n",
      "[213]\tvalidation_0-mlogloss:0.270286\tvalidation_1-mlogloss:0.360737\n",
      "[214]\tvalidation_0-mlogloss:0.270106\tvalidation_1-mlogloss:0.360717\n",
      "[215]\tvalidation_0-mlogloss:0.269897\tvalidation_1-mlogloss:0.360768\n",
      "[216]\tvalidation_0-mlogloss:0.26958\tvalidation_1-mlogloss:0.360743\n",
      "[217]\tvalidation_0-mlogloss:0.269354\tvalidation_1-mlogloss:0.360662\n",
      "[218]\tvalidation_0-mlogloss:0.269073\tvalidation_1-mlogloss:0.360657\n",
      "[219]\tvalidation_0-mlogloss:0.268695\tvalidation_1-mlogloss:0.360712\n",
      "[220]\tvalidation_0-mlogloss:0.268335\tvalidation_1-mlogloss:0.3606\n",
      "[221]\tvalidation_0-mlogloss:0.268005\tvalidation_1-mlogloss:0.360727\n",
      "[222]\tvalidation_0-mlogloss:0.267745\tvalidation_1-mlogloss:0.360657\n",
      "[223]\tvalidation_0-mlogloss:0.26746\tvalidation_1-mlogloss:0.360646\n",
      "[224]\tvalidation_0-mlogloss:0.267158\tvalidation_1-mlogloss:0.360703\n",
      "[225]\tvalidation_0-mlogloss:0.266976\tvalidation_1-mlogloss:0.360713\n",
      "[226]\tvalidation_0-mlogloss:0.266684\tvalidation_1-mlogloss:0.360655\n",
      "[227]\tvalidation_0-mlogloss:0.266448\tvalidation_1-mlogloss:0.360738\n",
      "[228]\tvalidation_0-mlogloss:0.266168\tvalidation_1-mlogloss:0.360778\n",
      "[229]\tvalidation_0-mlogloss:0.265828\tvalidation_1-mlogloss:0.360853\n",
      "[230]\tvalidation_0-mlogloss:0.265515\tvalidation_1-mlogloss:0.360855\n",
      "[231]\tvalidation_0-mlogloss:0.265089\tvalidation_1-mlogloss:0.360762\n",
      "[232]\tvalidation_0-mlogloss:0.26489\tvalidation_1-mlogloss:0.360761\n",
      "[233]\tvalidation_0-mlogloss:0.264605\tvalidation_1-mlogloss:0.360644\n",
      "[234]\tvalidation_0-mlogloss:0.264422\tvalidation_1-mlogloss:0.360612\n",
      "[235]\tvalidation_0-mlogloss:0.264109\tvalidation_1-mlogloss:0.360545\n",
      "[236]\tvalidation_0-mlogloss:0.26382\tvalidation_1-mlogloss:0.360528\n",
      "[237]\tvalidation_0-mlogloss:0.26347\tvalidation_1-mlogloss:0.360454\n",
      "[238]\tvalidation_0-mlogloss:0.263281\tvalidation_1-mlogloss:0.360498\n",
      "[239]\tvalidation_0-mlogloss:0.262914\tvalidation_1-mlogloss:0.360496\n",
      "[240]\tvalidation_0-mlogloss:0.262508\tvalidation_1-mlogloss:0.360404\n",
      "[241]\tvalidation_0-mlogloss:0.262149\tvalidation_1-mlogloss:0.360318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[242]\tvalidation_0-mlogloss:0.261845\tvalidation_1-mlogloss:0.360347\n",
      "[243]\tvalidation_0-mlogloss:0.261679\tvalidation_1-mlogloss:0.360347\n",
      "[244]\tvalidation_0-mlogloss:0.261543\tvalidation_1-mlogloss:0.360307\n",
      "[245]\tvalidation_0-mlogloss:0.261152\tvalidation_1-mlogloss:0.360268\n",
      "[246]\tvalidation_0-mlogloss:0.260805\tvalidation_1-mlogloss:0.360307\n",
      "[247]\tvalidation_0-mlogloss:0.260573\tvalidation_1-mlogloss:0.3604\n",
      "[248]\tvalidation_0-mlogloss:0.260254\tvalidation_1-mlogloss:0.360344\n",
      "[249]\tvalidation_0-mlogloss:0.260018\tvalidation_1-mlogloss:0.360445\n",
      "[250]\tvalidation_0-mlogloss:0.259777\tvalidation_1-mlogloss:0.360403\n",
      "[251]\tvalidation_0-mlogloss:0.259422\tvalidation_1-mlogloss:0.360376\n",
      "[252]\tvalidation_0-mlogloss:0.259124\tvalidation_1-mlogloss:0.360381\n",
      "[253]\tvalidation_0-mlogloss:0.258831\tvalidation_1-mlogloss:0.360429\n",
      "[254]\tvalidation_0-mlogloss:0.258574\tvalidation_1-mlogloss:0.360417\n",
      "[255]\tvalidation_0-mlogloss:0.258366\tvalidation_1-mlogloss:0.360408\n",
      "[256]\tvalidation_0-mlogloss:0.258072\tvalidation_1-mlogloss:0.360459\n",
      "[257]\tvalidation_0-mlogloss:0.257779\tvalidation_1-mlogloss:0.360436\n",
      "[258]\tvalidation_0-mlogloss:0.257495\tvalidation_1-mlogloss:0.360451\n",
      "[259]\tvalidation_0-mlogloss:0.257112\tvalidation_1-mlogloss:0.360372\n",
      "[260]\tvalidation_0-mlogloss:0.256938\tvalidation_1-mlogloss:0.36042\n",
      "[261]\tvalidation_0-mlogloss:0.256708\tvalidation_1-mlogloss:0.360468\n",
      "[262]\tvalidation_0-mlogloss:0.256474\tvalidation_1-mlogloss:0.360404\n",
      "[263]\tvalidation_0-mlogloss:0.256132\tvalidation_1-mlogloss:0.360462\n",
      "[264]\tvalidation_0-mlogloss:0.255863\tvalidation_1-mlogloss:0.360503\n",
      "[265]\tvalidation_0-mlogloss:0.255661\tvalidation_1-mlogloss:0.36055\n",
      "[266]\tvalidation_0-mlogloss:0.255373\tvalidation_1-mlogloss:0.360548\n",
      "[267]\tvalidation_0-mlogloss:0.255102\tvalidation_1-mlogloss:0.360484\n",
      "[268]\tvalidation_0-mlogloss:0.254892\tvalidation_1-mlogloss:0.360549\n",
      "[269]\tvalidation_0-mlogloss:0.254685\tvalidation_1-mlogloss:0.360637\n",
      "[270]\tvalidation_0-mlogloss:0.254476\tvalidation_1-mlogloss:0.360739\n",
      "[271]\tvalidation_0-mlogloss:0.254155\tvalidation_1-mlogloss:0.360806\n",
      "[272]\tvalidation_0-mlogloss:0.253835\tvalidation_1-mlogloss:0.360823\n",
      "[273]\tvalidation_0-mlogloss:0.253594\tvalidation_1-mlogloss:0.360874\n",
      "[274]\tvalidation_0-mlogloss:0.253306\tvalidation_1-mlogloss:0.360932\n",
      "[275]\tvalidation_0-mlogloss:0.253063\tvalidation_1-mlogloss:0.360953\n",
      "[276]\tvalidation_0-mlogloss:0.252823\tvalidation_1-mlogloss:0.361027\n",
      "[277]\tvalidation_0-mlogloss:0.25254\tvalidation_1-mlogloss:0.361002\n",
      "[278]\tvalidation_0-mlogloss:0.252188\tvalidation_1-mlogloss:0.36096\n",
      "[279]\tvalidation_0-mlogloss:0.251991\tvalidation_1-mlogloss:0.360977\n",
      "[280]\tvalidation_0-mlogloss:0.251785\tvalidation_1-mlogloss:0.360998\n",
      "[281]\tvalidation_0-mlogloss:0.251499\tvalidation_1-mlogloss:0.361001\n",
      "[282]\tvalidation_0-mlogloss:0.251395\tvalidation_1-mlogloss:0.361065\n",
      "[283]\tvalidation_0-mlogloss:0.251134\tvalidation_1-mlogloss:0.3611\n",
      "[284]\tvalidation_0-mlogloss:0.250868\tvalidation_1-mlogloss:0.36098\n",
      "[285]\tvalidation_0-mlogloss:0.250564\tvalidation_1-mlogloss:0.361063\n",
      "[286]\tvalidation_0-mlogloss:0.250321\tvalidation_1-mlogloss:0.361155\n",
      "[287]\tvalidation_0-mlogloss:0.249964\tvalidation_1-mlogloss:0.361204\n",
      "[288]\tvalidation_0-mlogloss:0.249624\tvalidation_1-mlogloss:0.361234\n",
      "[289]\tvalidation_0-mlogloss:0.249358\tvalidation_1-mlogloss:0.36126\n",
      "[290]\tvalidation_0-mlogloss:0.249088\tvalidation_1-mlogloss:0.361332\n",
      "[291]\tvalidation_0-mlogloss:0.248807\tvalidation_1-mlogloss:0.361424\n",
      "[292]\tvalidation_0-mlogloss:0.248523\tvalidation_1-mlogloss:0.361571\n",
      "[293]\tvalidation_0-mlogloss:0.248248\tvalidation_1-mlogloss:0.361406\n",
      "[294]\tvalidation_0-mlogloss:0.247999\tvalidation_1-mlogloss:0.361303\n",
      "[295]\tvalidation_0-mlogloss:0.247736\tvalidation_1-mlogloss:0.361268\n",
      "Stopping. Best iteration:\n",
      "[245]\tvalidation_0-mlogloss:0.261152\tvalidation_1-mlogloss:0.360268\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, sub_sample=0.8, subsample=1)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB.fit(X_training_stack,Y_training_stack,eval_set=[(X_training_stack,Y_training_stack),(X_test_stack,Y_test_stack)],eval_metric='mlogloss',early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with learning rate on 0.1, we can see that the 245 trees is giving the best result, using this we will tune for max_depth and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB1=XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=245,silent=False,objective='multi:softmax',sub_sample=0.8,colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid={'sub_sample':[0.5,0.6],'colsample_bytree':[0.5,0.6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GridSearchCV(XGB1,param_grid=parameter_grid,scoring='log_loss',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=245,\n",
       "       n_jobs=1, nthread=None, objective='multi:softmax', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, sub_sample=0.8, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'colsample_bytree': [0.5, 0.6], 'sub_sample': [0.5, 0.6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='log_loss', verbose=0)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_training_stack,Y_training_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 8.93824382, 31.25310559, 10.3675189 , 10.42822018]),\n",
       " 'mean_score_time': array([0.08146119, 0.08166819, 0.08077669, 0.08022943]),\n",
       " 'mean_test_score': array([-0.34050062, -0.34050062, -0.34181932, -0.34181932]),\n",
       " 'mean_train_score': array([-0.25376899, -0.25376899, -0.2525625 , -0.2525625 ]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.6, 0.6],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_sub_sample': masked_array(data=[0.5, 0.6, 0.5, 0.6],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5, 'sub_sample': 0.5},\n",
       "  {'colsample_bytree': 0.5, 'sub_sample': 0.6},\n",
       "  {'colsample_bytree': 0.6, 'sub_sample': 0.5},\n",
       "  {'colsample_bytree': 0.6, 'sub_sample': 0.6}],\n",
       " 'rank_test_score': array([1, 1, 3, 3], dtype=int32),\n",
       " 'split0_test_score': array([-0.33030001, -0.33030001, -0.33240457, -0.33240457]),\n",
       " 'split0_train_score': array([-0.25475796, -0.25475796, -0.2533583 , -0.2533583 ]),\n",
       " 'split1_test_score': array([-0.34011638, -0.34011638, -0.34130999, -0.34130999]),\n",
       " 'split1_train_score': array([-0.2530616 , -0.2530616 , -0.25262732, -0.25262732]),\n",
       " 'split2_test_score': array([-0.34245827, -0.34245827, -0.34247644, -0.34247644]),\n",
       " 'split2_train_score': array([-0.25462125, -0.25462125, -0.25302643, -0.25302643]),\n",
       " 'split3_test_score': array([-0.34597918, -0.34597918, -0.34821414, -0.34821414]),\n",
       " 'split3_train_score': array([-0.25330742, -0.25330742, -0.252471  , -0.252471  ]),\n",
       " 'split4_test_score': array([-0.34365567, -0.34365567, -0.3446978 , -0.3446978 ]),\n",
       " 'split4_train_score': array([-0.25309671, -0.25309671, -0.25132946, -0.25132946]),\n",
       " 'std_fit_time': array([1.92045321e-01, 4.46557078e+01, 3.33042935e-02, 1.11742281e-01]),\n",
       " 'std_score_time': array([0.00070888, 0.00091026, 0.00110562, 0.00088241]),\n",
       " 'std_test_score': array([0.0054412 , 0.0054412 , 0.00526367, 0.00526367]),\n",
       " 'std_train_score': array([0.0007576 , 0.0007576 , 0.00069014, 0.00069014])}"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cv score: -0.34 best parameters: {'colsample_bytree': 0.5, 'sub_sample': 0.5} \n"
     ]
    }
   ],
   "source": [
    "print(\"best cv score: %.2f best parameters: %s \"%(clf.best_score_,clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36041591598071"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test_stack,clf.predict_proba(X_test_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8602655771195097"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test_stack,clf.predict(X_test_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using stacked models has improved logloss score from 0.48 to 0.36, but the over all accuracy improvement is marginal only 4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Summary **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For author identification, we have taken multi pronged approach to tackle the problem and finally stacked all the models together , which has improved accuracy and logloss score.\n",
    "\n",
    "Tfidf vectorizer with Naive bayes algorithm : Accuracy 81.2% logloss score:0.460\n",
    "\n",
    "Cvt vectorizer with Naive bayes algorithm : Accuracy 83.9% logloss score: 0.480\n",
    "\n",
    "Markov chain with order 3 : Accuracy Accuracy 78.9% logloss score:1.05\n",
    "We have used LSI to reduce the dimensionality and this can be used build a independant model, Instead we have used it to stack \n",
    "with other models.\n",
    "\n",
    "Finally we have stack LSI,Tfidf,Cvt,LSI features and applied XGboost , which has resulted in highest accuracy:86% and highest logloss score:0.36\n",
    "\n",
    "For practical purposes, cvt model will be sufficient because it's accuracy is only 2% less and the model is simple,unlike the stacked model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy can be further improved by trying the following options:\n",
    "    \n",
    "    1) Tune min_df and max_df parameters in Count vectorizer and TfIdf vectorizer\n",
    "    \n",
    "    2) Tune the number of dimensions in LSI using an independant model\n",
    "    \n",
    "    3) In the markov model we assumed order 3 and alpha of 0.1. These parameters can be tuned further for improved accuracy\n",
    "    \n",
    "Over all the classifier built above serves practical purposes , but can be further improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
